{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f5174e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89178a50",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Traffic Distribution System (TDS) Simulation and Monitoring\n",
    "\n",
    "This notebook simulates a traffic distribution system for affiliate marketing, including:\n",
    "- Data loading and preprocessing from CSVs (Clicks, Postbacks, Offers, Sources)\n",
    "- Feature engineering for model training\n",
    "- RandomForest-based offer selection (with epsilon-greedy exploration)\n",
    "- KPI monitoring and anomaly detection (EPC, CR, CTR, suspicious IPs, traffic spikes)\n",
    "- Dynamic offer cap management (daily/total, by payout type)\n",
    "- Visualization of KPI history\n",
    "\n",
    "Key files:\n",
    "- selection_history.csv: Offer assignment log\n",
    "- kpi_monitoring.csv: Daily KPI metrics\n",
    "- alerts_log.csv: Anomaly alerts\n",
    "\n",
    "Main functions:\n",
    "- monitor_kpi: Calculate and log daily KPIs\n",
    "- check_anomalies: Detect and log traffic/conversion anomalies\n",
    "- plot_kpi_history: Visualize recent KPI trends\n",
    "- update_caps_dynamic: Update offer caps based on traffic/conversions\n",
    "- choose_offer: Select best offer for a click (model-based, with exploration)\n",
    "- run_simulation: Run a simulation for a sample of clicks\n",
    "\n",
    "Dependencies: pandas, numpy, scikit-learn, matplotlib, collections, os, gc, datetime\n",
    "\n",
    "Author:  Ihor Ivanyshyn\n",
    "Date: 2025-08-10\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# File paths (update if running outside Colab)\n",
    "POSTBACKS_PATH = \"/content/drive/MyDrive/TDS/Postbacks.csv\"\n",
    "CLICKS_PATH = \"/content/drive/MyDrive/TDS/Clicks.csv\"\n",
    "SOURCES_PATH = \"/content/drive/MyDrive/TDS/Source.csv\"\n",
    "OFFERS_PATH = \"/content/drive/MyDrive/TDS/Offers.csv\"\n",
    "\n",
    "OUTPUT_SELECTION_HISTORY = \"selection_history.csv\"\n",
    "KPI_MONITORING_PATH = \"kpi_monitoring.csv\"\n",
    "ALERTS_LOG_PATH = \"alerts_log.csv\"\n",
    "\n",
    "print(\"[INFO] Завантаження даних...\")  # Loading data\n",
    "# Load and shuffle click/postback data for randomness\n",
    "postbacks = pd.read_csv(POSTBACKS_PATH).sample(frac=1, random_state=42)\n",
    "clicks = pd.read_csv(CLICKS_PATH).sample(frac=1, random_state=42)\n",
    "sources = pd.read_csv(SOURCES_PATH)\n",
    "offers = pd.read_csv(OFFERS_PATH)\n",
    "\n",
    "print(f\"[INFO] Rows: postbacks={len(postbacks)}, clicks={len(clicks)}, sources={len(sources)}, offers={len(offers)}\")\n",
    "\n",
    "# Convert timestamp columns to datetime if needed\n",
    "for df, col in [(clicks, 'click_timestamp'), (postbacks, 'postback_timestamp')]:\n",
    "    if col in df.columns and not np.issubdtype(df[col].dtype, np.datetime64):\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "# Filter postbacks to only those with valid click_id in clicks\n",
    "postbacks = postbacks[postbacks['click_id'].isin(clicks['click_id'])].copy()\n",
    "\n",
    "# Normalize OS column\n",
    "if 'os_type' in clicks.columns and 'os' not in clicks.columns:\n",
    "    clicks['os'] = clicks['os_type']\n",
    "\n",
    "# Create unique key for each click (user_id + click_number or fallback to click_id)\n",
    "if 'user_id' in clicks.columns and 'click_number' in clicks.columns:\n",
    "    clicks['unique_key'] = clicks['user_id'].astype(str) + '_' + clicks['click_number'].astype(str)\n",
    "else:\n",
    "    clicks['unique_key'] = clicks['click_id']\n",
    "\n",
    "# Remove duplicate clicks within 1 second for the same user\n",
    "if 'user_id' in clicks.columns and 'click_timestamp' in clicks.columns:\n",
    "    clicks = clicks.sort_values(['user_id', 'click_timestamp'])\n",
    "    clicks['time_diff'] = clicks.groupby('user_id')['click_timestamp'].diff().dt.total_seconds()\n",
    "    clicks = clicks[clicks['time_diff'].isna() | (clicks['time_diff'] >= 1)].drop('time_diff', axis=1)\n",
    "\n",
    "# Merge source and offer info into clicks\n",
    "clicks = clicks.merge(sources, on='source_id', how='left')\n",
    "clicks = clicks.merge(offers, on='offer_id', how='left')\n",
    "\n",
    "# Calculate EPC (Earnings Per Click) per offer\n",
    "epc_df = postbacks.groupby('offer_id')['revenue'].mean().reset_index().rename(columns={'revenue': 'EPC'})\n",
    "clicks = clicks.merge(epc_df, on='offer_id', how='left').fillna({'EPC': 0})\n",
    "baseline_epc = epc_df['EPC'].mean()\n",
    "\n",
    "# Calculate CR (Conversion Rate) per offer\n",
    "cr_series = postbacks.groupby('offer_id').size() / clicks.groupby('offer_id').size()\n",
    "cr_df = cr_series.reset_index().rename(columns={0: 'CR'}).fillna(0)\n",
    "clicks = clicks.merge(cr_df, on='offer_id', how='left').fillna({'CR': 0})\n",
    "\n",
    "# Encode categorical columns for model input\n",
    "categorical_cols = ['browser', 'placement', 'device_type', 'geo', 'os', 'network', 'payout_type']\n",
    "encoders = {}\n",
    "for col in categorical_cols:\n",
    "    if col in clicks.columns:\n",
    "        le = LabelEncoder()\n",
    "        clicks[col] = le.fit_transform(clicks[col].astype(str))\n",
    "        encoders[col] = le\n",
    "\n",
    "# Ensure offer cap columns exist\n",
    "if 'remaining_daily_cap' not in offers.columns:\n",
    "    offers['remaining_daily_cap'] = offers['daily_cap_amount'].fillna(1e12)\n",
    "if 'remaining_total_cap' not in offers.columns:\n",
    "    offers['remaining_total_cap'] = offers['total_cap_amount'].fillna(1e12)\n",
    "\n",
    "print(\"[INFO] Тренування RandomForest...\")  # Training RandomForest\n",
    "# Model features and target\n",
    "features = [c for c in ['browser', 'placement', 'device_type', 'geo', 'os', 'network', 'payout_type', 'EPC', 'CR'] if c in clicks.columns]\n",
    "clicks['target'] = clicks['offer_id'].isin(postbacks['offer_id']).astype(int)\n",
    "\n",
    "X = clicks[features]\n",
    "y = clicks['target']\n",
    "\n",
    "# Train RandomForest model for offer selection\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "model.fit(X, y)\n",
    "print(f\"[INFO] Модель натренована. Позитивних прикладів: {y.sum()}\")\n",
    "gc.collect()\n",
    "\n",
    "# KPI history windows for plotting\n",
    "epc_window = deque(maxlen=50)\n",
    "cr_window = deque(maxlen=50)\n",
    "ctr_window = deque(maxlen=50)\n",
    "\n",
    "def monitor_kpi(clicks_df, postbacks_df, current_date):\n",
    "    \"\"\"\n",
    "    Calculate and log daily KPIs (Clicks, Conversions, Impressions, CR, CTR, EPC, Revenue).\n",
    "    Appends results to KPI_MONITORING_PATH and updates rolling windows for plotting.\n",
    "    \"\"\"\n",
    "    daily_clicks = clicks_df[clicks_df['click_timestamp'].dt.date == current_date]\n",
    "    daily_postbacks = postbacks_df[postbacks_df['postback_timestamp'].dt.date == current_date]\n",
    "\n",
    "    clicks_count = len(daily_clicks)\n",
    "    conversions = len(daily_postbacks)\n",
    "    impressions = clicks_count * 10  # 1 click = 10 impressions\n",
    "    cr = (conversions / clicks_count * 100) if clicks_count > 0 else 0\n",
    "    ctr = (clicks_count / impressions * 100) if impressions > 0 else 0\n",
    "    revenue = daily_postbacks['revenue'].sum()\n",
    "    epc = revenue / clicks_count if clicks_count > 0 else 0\n",
    "\n",
    "    kpi = {\n",
    "        'date': current_date,\n",
    "        'clicks': clicks_count,\n",
    "        'conversions': conversions,\n",
    "        'impressions': impressions,\n",
    "        'cr_percent': cr,\n",
    "        'ctr_percent': ctr,\n",
    "        'epc': epc,\n",
    "        'revenue': revenue\n",
    "    }\n",
    "\n",
    "    # Append to KPI log file\n",
    "    pd.DataFrame([kpi]).to_csv(KPI_MONITORING_PATH, mode='a', header=not os.path.exists(KPI_MONITORING_PATH), index=False)\n",
    "    print(f\"[KPI] Date: {current_date}, Clicks: {clicks_count}, Impressions: {impressions}, CR: {cr:.2f}%, CTR: {ctr:.2f}%, EPC: {epc:.4f}\")\n",
    "\n",
    "    # Update rolling windows for plotting\n",
    "    epc_window.append(epc)\n",
    "    cr_window.append(cr)\n",
    "    ctr_window.append(ctr)\n",
    "\n",
    "    return kpi\n",
    "\n",
    "def check_anomalies(clicks_df, postbacks_df, baseline_epc, current_date):\n",
    "    \"\"\"\n",
    "    Detects and logs anomalies:\n",
    "    - Traffic spikes (clicks per hour)\n",
    "    - Low EPC\n",
    "    - High CR\n",
    "    - Suspicious IPs (many clicks from one IP)\n",
    "    Appends alerts to ALERTS_LOG_PATH and prints recommended actions.\n",
    "    \"\"\"\n",
    "    alerts = []\n",
    "    daily_clicks = clicks_df[clicks_df['click_timestamp'].dt.date == current_date]\n",
    "    daily_postbacks = postbacks_df[postbacks_df['postback_timestamp'].dt.date == current_date]\n",
    "\n",
    "    # Traffic spike detection\n",
    "    clicks_per_hour = daily_clicks.groupby(daily_clicks['click_timestamp'].dt.hour).size()\n",
    "    mean_clicks = clicks_per_hour.mean()\n",
    "    std_clicks = clicks_per_hour.std()\n",
    "    threshold = mean_clicks + 3 * std_clicks if std_clicks > 0 else mean_clicks * 2\n",
    "    for hour, count in clicks_per_hour.items():\n",
    "        if count > threshold:\n",
    "            alerts.append({'timestamp': datetime.utcnow(), 'type': 'traffic_spike', 'details': f'Hour {hour}: {count} clicks > {threshold:.0f}'})\n",
    "\n",
    "    # Low EPC detection\n",
    "    clicks_count = len(daily_clicks)\n",
    "    revenue = daily_postbacks['revenue'].sum()\n",
    "    epc = revenue / clicks_count if clicks_count > 0 else 0\n",
    "    if epc < baseline_epc * 0.8 and clicks_count > 100:\n",
    "        alerts.append({'timestamp': datetime.utcnow(), 'type': 'low_epc', 'details': f'EPC {epc:.4f} < {baseline_epc * 0.8:.4f} (-20%)'})\n",
    "\n",
    "    # High CR detection\n",
    "    conversions = len(daily_postbacks)\n",
    "    cr = (conversions / clicks_count * 100) if clicks_count > 0 else 0\n",
    "    if cr > 50 and clicks_count > 100:\n",
    "        alerts.append({'timestamp': datetime.utcnow(), 'type': 'high_cr', 'details': f'CR {cr:.2f}% > 50%'})\n",
    "\n",
    "    # Suspicious IP detection\n",
    "    if 'ip_address' in daily_clicks.columns:\n",
    "        ip_counts = daily_clicks.groupby('ip_address').size()\n",
    "        for ip, count in ip_counts.items():\n",
    "            if count > 100:\n",
    "                alerts.append({'timestamp': datetime.utcnow(), 'type': 'suspicious_ip', 'details': f'IP {ip}: {count} clicks'})\n",
    "\n",
    "    # Log and print alerts\n",
    "    if alerts:\n",
    "        pd.DataFrame(alerts).to_csv(ALERTS_LOG_PATH, mode='a', header=not os.path.exists(ALERTS_LOG_PATH), index=False)\n",
    "        for alert in alerts:\n",
    "            print(f\"[ALERT] {alert['type']}: {alert['details']}\")\n",
    "            if alert['type'] == 'suspicious_ip':\n",
    "                print(f\"[ACTION] Blocking IP: {alert['details'].split(':')[0].split(' ')[1]}\")\n",
    "            elif alert['type'] == 'traffic_spike':\n",
    "                print(\"[ACTION] Investigate traffic source for potential fraud\")\n",
    "            elif alert['type'] in ['low_epc', 'high_cr']:\n",
    "                print(\"[ACTION] Review offer performance and traffic quality\")\n",
    "\n",
    "    return alerts\n",
    "\n",
    "def plot_kpi_history():\n",
    "    \"\"\"\n",
    "    Plots the recent history (last 50) of EPC, CR, and CTR using matplotlib.\n",
    "    Shows alert thresholds for EPC and CR.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(range(len(epc_window)), epc_window, label='EPC', color='green')\n",
    "    plt.axhline(y=baseline_epc * 0.8, color='red', linestyle='--', label='EPC Alert Threshold (-20%)')\n",
    "    plt.title('EPC (Last 50 clicks)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(range(len(cr_window)), cr_window, label='CR (%)', color='blue')\n",
    "    plt.axhline(y=50, color='red', linestyle='--', label='CR Alert Threshold (50%)')\n",
    "    plt.title('Conversion Rate (CR)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(range(len(ctr_window)), ctr_window, label='CTR (%)', color='orange')\n",
    "    plt.title('Click-Through Rate (CTR)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def update_caps_dynamic(offers_df, clicks_df, postbacks_df, current_date):\n",
    "    \"\"\"\n",
    "    Updates remaining_daily_cap and remaining_total_cap for each offer based on:\n",
    "    - PPC: counts clicks\n",
    "    - PPL/PPS: counts conversions\n",
    "    Returns a new offers DataFrame with updated caps.\n",
    "    \"\"\"\n",
    "    offers_df = offers_df.copy()\n",
    "    daily_clicks = clicks_df[clicks_df['click_timestamp'].dt.date == current_date] if not clicks_df.empty else pd.DataFrame()\n",
    "    daily_pbs = postbacks_df[postbacks_df['postback_timestamp'].dt.date == current_date] if not postbacks_df.empty else pd.DataFrame()\n",
    "\n",
    "    # Update PPC offers (by clicks)\n",
    "    ppc_mask = offers_df['payout_type'] == 'PPC'\n",
    "    if ppc_mask.any():\n",
    "        ppc_offer_ids = offers_df.loc[ppc_mask, 'offer_id'].unique()\n",
    "        daily_click_counts = daily_clicks[daily_clicks['offer_id'].isin(ppc_offer_ids)].groupby('offer_id').size() if not daily_clicks.empty else pd.Series()\n",
    "        total_click_counts = clicks_df[clicks_df['offer_id'].isin(ppc_offer_ids)].groupby('offer_id').size() if not clicks_df.empty else pd.Series()\n",
    "        for oid in ppc_offer_ids:\n",
    "            dcount = daily_click_counts.get(oid, 0)\n",
    "            tcount = total_click_counts.get(oid, 0)\n",
    "            daily_cap = offers_df.loc[offers_df['offer_id'] == oid, 'daily_cap_amount'].fillna(1e12).values[0]\n",
    "            total_cap = offers_df.loc[offers_df['offer_id'] == oid, 'total_cap_amount'].fillna(1e12).values[0]\n",
    "            offers_df.loc[offers_df['offer_id'] == oid, 'remaining_daily_cap'] = max(0, daily_cap - dcount)\n",
    "            offers_df.loc[offers_df['offer_id'] == oid, 'remaining_total_cap'] = max(0, total_cap - tcount)\n",
    "\n",
    "    # Update PPL/PPS offers (by conversions)\n",
    "    ppl_mask = offers_df['payout_type'].isin(['PPL', 'PPS'])\n",
    "    if ppl_mask.any():\n",
    "        ppl_offer_ids = offers_df.loc[ppl_mask, 'offer_id'].unique()\n",
    "        daily_conv_counts = daily_pbs[daily_pbs['offer_id'].isin(ppl_offer_ids)].groupby('offer_id').size() if not daily_pbs.empty else pd.Series()\n",
    "        total_conv_counts = postbacks_df[postbacks_df['offer_id'].isin(ppl_offer_ids)].groupby('offer_id').size() if not postbacks_df.empty else pd.Series()\n",
    "        for oid in ppl_offer_ids:\n",
    "            dconv = daily_conv_counts.get(oid, 0)\n",
    "            tconv = total_conv_counts.get(oid, 0)\n",
    "            daily_cap = offers_df.loc[offers_df['offer_id'] == oid, 'daily_cap_amount'].fillna(1e12).values[0]\n",
    "            total_cap = offers_df.loc[offers_df['offer_id'] == oid, 'total_cap_amount'].fillna(1e12).values[0]\n",
    "            offers_df.loc[offers_df['offer_id'] == oid, 'remaining_daily_cap'] = max(0, daily_cap - dconv)\n",
    "            offers_df.loc[offers_df['offer_id'] == oid, 'remaining_total_cap'] = max(0, total_cap - tconv)\n",
    "\n",
    "    gc.collect()\n",
    "    return offers_df\n",
    "\n",
    "def choose_offer(click_row, epsilon=0.1, offers_df=offers):\n",
    "    \"\"\"\n",
    "    Selects the best offer for a given click using a trained model and epsilon-greedy exploration.\n",
    "    - Filters offers by cap and pool type (prioritized/active/backfill)\n",
    "    - Predicts conversion probability for each offer\n",
    "    - With probability (1-epsilon), picks best; with epsilon, samples from top 3\n",
    "    - Updates offer caps accordingly\n",
    "    - Logs selection to OUTPUT_SELECTION_HISTORY\n",
    "    Returns: (selected_offer_id, info_dict)\n",
    "    \"\"\"\n",
    "    is_split = click_row.get('is_split_offer', 0)\n",
    "    offers_df = update_caps_dynamic(offers_df, clicks, postbacks, click_row['click_timestamp'].date())\n",
    "    if is_split == 0:\n",
    "        possible_offers = offers_df[(offers_df['is_backfill'] == 0) & (offers_df['remaining_daily_cap'] > 0) & (offers_df['remaining_total_cap'] > 0)]\n",
    "    else:\n",
    "        possible_offers = offers_df[(offers_df['remaining_daily_cap'] > 0) & (offers_df['remaining_total_cap'] > 0)]\n",
    "\n",
    "    if possible_offers.empty:\n",
    "        possible_offers = offers_df[(offers_df['is_backfill'] == 1) & (offers_df['remaining_daily_cap'] > 0) & (offers_df['remaining_total_cap'] > 0)]\n",
    "\n",
    "    if possible_offers.empty:\n",
    "        return None, {'reason': 'no_offers'}\n",
    "\n",
    "    possible_offers = possible_offers.copy()\n",
    "    input_data = []\n",
    "    for _, off in possible_offers.iterrows():\n",
    "        row_data = click_row[features].copy()\n",
    "        row_data['EPC'] = epc_df.loc[epc_df['offer_id'] == off['offer_id'], 'EPC'].mean() if off['offer_id'] in epc_df['offer_id'].values else 0\n",
    "        row_data['CR'] = cr_df.loc[cr_df['offer_id'] == off['offer_id'], 'CR'].mean() if off['offer_id'] in cr_df['offer_id'].values else 0\n",
    "        input_data.append(row_data)\n",
    "\n",
    "    input_df = pd.DataFrame(input_data, columns=features)\n",
    "    preds = model.predict_proba(input_df)[:, 1]\n",
    "    possible_offers['score'] = preds\n",
    "\n",
    "    # Epsilon-greedy selection\n",
    "    if np.random.rand() > epsilon:\n",
    "        best_offer = possible_offers.loc[possible_offers['score'].idxmax()]\n",
    "    else:\n",
    "        top3 = possible_offers.nlargest(3, 'score')\n",
    "        best_offer = top3.sample(1).iloc[0]\n",
    "\n",
    "    selected_offer_id = best_offer['offer_id']\n",
    "\n",
    "    # Update caps in global offers DataFrame\n",
    "    idx_offer = offers.index[offers['offer_id'] == selected_offer_id]\n",
    "    if not idx_offer.empty:\n",
    "        idx_offer = idx_offer[0]\n",
    "        if best_offer['payout_type'] == 'PPC':\n",
    "            offers.at[idx_offer, 'remaining_daily_cap'] = max(0, offers.at[idx_offer, 'remaining_daily_cap'] - 1)\n",
    "            offers.at[idx_offer, 'remaining_total_cap'] = max(0, offers.at[idx_offer, 'remaining_total_cap'] - 1)\n",
    "        else:\n",
    "            had_pb = (postbacks['click_id'] == click_row['click_id']).any()\n",
    "            if had_pb:\n",
    "                offers.at[idx_offer, 'remaining_daily_cap'] = max(0, offers.at[idx_offer, 'remaining_daily_cap'] - 1)\n",
    "                offers.at[idx_offer, 'remaining_total_cap'] = max(0, offers.at[idx_offer, 'remaining_total_cap'] - 1)\n",
    "\n",
    "    info = {\n",
    "        'pool_type': 'prioritized' if is_split == 0 else 'active' if not possible_offers['is_backfill'].all() else 'backfill',\n",
    "        'score': best_offer['score'],\n",
    "        'epsilon': epsilon\n",
    "    }\n",
    "    rec = {\n",
    "        'timestamp': datetime.utcnow(),\n",
    "        'click_id': click_row['click_id'],\n",
    "        'assigned_offer': selected_offer_id,\n",
    "        'pool_type': info['pool_type'],\n",
    "        'score': info['score']\n",
    "    }\n",
    "    pd.DataFrame([rec]).to_csv(OUTPUT_SELECTION_HISTORY, mode='a', header=not os.path.exists(OUTPUT_SELECTION_HISTORY), index=False)\n",
    "\n",
    "    return selected_offer_id, info\n",
    "\n",
    "def run_simulation(sample_size=1, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Runs a simulation for a sample of clicks:\n",
    "    - For each click, selects an offer and logs assignment\n",
    "    - Monitors KPIs and checks for anomalies\n",
    "    - Plots KPI history at the end\n",
    "    \"\"\"\n",
    "    sampled_clicks = clicks.sample(sample_size, random_state=42).reset_index(drop=True)\n",
    "    current_date = datetime(year=2024, month=10, day=17).date()\n",
    "\n",
    "    for i, click_row in sampled_clicks.iterrows():\n",
    "        chosen_offer, info = choose_offer(click_row, epsilon)\n",
    "\n",
    "        monitor_kpi(clicks, postbacks, current_date)\n",
    "        check_anomalies(clicks, postbacks, baseline_epc, current_date)\n",
    "\n",
    "    plot_kpi_history()\n",
    "\n",
    "run_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c766ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(sample_size=1, epsilon=0.1):\n",
    "    sampled_clicks = clicks.sample(sample_size, random_state=42).reset_index(drop=True)\n",
    "    current_date = datetime(year=2024, month=10, day=17).date()\n",
    "\n",
    "    for i, click_row in sampled_clicks.iterrows():\n",
    "        chosen_offer, info = choose_offer(click_row, epsilon)\n",
    "\n",
    "        print(click_row,chosen_offer, info)\n",
    "\n",
    "run_simulation()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
